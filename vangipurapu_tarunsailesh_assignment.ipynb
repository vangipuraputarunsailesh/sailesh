{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jDyTKYs-yGit"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "def scrape_amazon_reviews(product_url, num_reviews):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "    reviews = []\n",
        "    for page_num in range(1, (num_reviews // 10) + 2):\n",
        "        url = f\"{product_url}/product-reviews/?pageNumber={page_num}\"\n",
        "        response = requests.get(\"https://www.amazon.com/Echo-Dot-3rd-Gen-Smart-Speaker-with-Alexa-Charcoal/dp/B07FZ8S74R/\", headers=headers)\n",
        "        if response.status_code != 200:\n",
        "            print(\"Failed to fetch page:\", page_num)\n",
        "            continue\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        review_blocks = soup.find_all('div', {'data-hook': 'review'})\n",
        "        for review_block in review_blocks:\n",
        "            review_text = review_block.find('span', {'data-hook': 'review-body'}).text.strip()\n",
        "            reviews.append(review_text)\n",
        "    return reviews\n",
        "def save_to_csv(data, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Review\"])\n",
        "        for item in data:\n",
        "            writer.writerow([item])\n",
        "if __name__ == \"__main__\":\n",
        "    product_url = \"YOUR_AMAZON_PRODUCT_URL\"\n",
        "    num_reviews = 1000\n",
        "    reviews = scrape_amazon_reviews(product_url, num_reviews)\n",
        "    save_to_csv(reviews, \"amazon_reviews.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBDifhG8-WuL",
        "outputId": "deb5f0b4-fc84-4d72-e489-aceeff50de0d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf2b1d80-732c-4c62-e45b-041e671cd5b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Write code for each of the sub parts with proper comments.\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "def scrape_amazon_reviews(product_url, num_reviews):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
        "    reviews = []\n",
        "    for page_num in range(1, (num_reviews // 10) + 2):\n",
        "        url = f\"{product_url}/product-reviews/?pageNumber={page_num}\"\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code != 200:\n",
        "            print(\"Failed to fetch page:\", page_num)\n",
        "            continue\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        review_blocks = soup.find_all('div', {'data-hook': 'review'})\n",
        "        for review_block in review_blocks:\n",
        "            review_text = review_block.find('span', {'data-hook': 'review-body'}).text.strip()\n",
        "            reviews.append(review_text)\n",
        "    return reviews\n",
        "def remove_noise(text):\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return cleaned_text\n",
        "def remove_numbers(text):\n",
        "    cleaned_text = re.sub(r'\\d+', '', text)\n",
        "    return cleaned_text\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_text)\n",
        "def lowercase(text):\n",
        "    return text.lower()\n",
        "def stem(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_text = ' '.join([stemmer.stem(word) for word in word_tokenize(text)])\n",
        "    return stemmed_text\n",
        "def lemmatize(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(text)])\n",
        "    return lemmatized_text\n",
        "def clean_text_data(product_url, num_reviews):\n",
        "    reviews = scrape_amazon_reviews(product_url, num_reviews)\n",
        "    cleaned_reviews = []\n",
        "    for review in reviews:\n",
        "        cleaned_review = review\n",
        "        cleaned_review = remove_noise(cleaned_review)\n",
        "        cleaned_review = remove_numbers(cleaned_review)\n",
        "        cleaned_review = remove_stopwords(cleaned_review)\n",
        "        cleaned_review = lowercase(cleaned_review)\n",
        "        cleaned_review = stem(cleaned_review)\n",
        "        cleaned_review = lemmatize(cleaned_review)\n",
        "        cleaned_reviews.append(cleaned_review)\n",
        "    with open(\"cleaned_amazon_reviews.csv\", 'w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Original Review\", \"Cleaned Review\"])\n",
        "        for original, cleaned in zip(reviews, cleaned_reviews):\n",
        "            writer.writerow([original, cleaned])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    product_url = \"https://www.amazon.com/Echo-Dot-3rd-Gen-Smart-Speaker-with-Alexa-Charcoal/dp/B07FZ8S74R/\"\n",
        "    num_reviews = 1000\n",
        "    clean_text_data(product_url, num_reviews)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7EgwVleAgQw",
        "outputId": "ae5e469b-45c0-4e49-b341-d36d39e0f4b2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag, ne_chunk\n",
        "import spacy\n",
        "\n",
        "def pos_tagging(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "\n",
        "    pos_count = {'Noun': 0, 'Verb': 0, 'Adjective': 0, 'Adverb': 0}\n",
        "    for _, tag in pos_tags:\n",
        "        if tag.startswith('N'):\n",
        "            pos_count['Noun'] += 1\n",
        "        elif tag.startswith('V'):\n",
        "            pos_count['Verb'] += 1\n",
        "        elif tag.startswith('J'):\n",
        "            pos_count['Adjective'] += 1\n",
        "        elif tag.startswith('R'):\n",
        "            pos_count['Adverb'] += 1\n",
        "\n",
        "    return pos_tags, pos_count\n",
        "\n",
        "def constituency_parsing(text, nlp):\n",
        "    doc = nlp(text)\n",
        "    for sentence in doc.sents:\n",
        "        print(sentence._.parse_string)\n",
        "\n",
        "def dependency_parsing(text, nlp):\n",
        "    doc = nlp(text)\n",
        "    for sentence in doc.sents:\n",
        "        print(sentence._.to_dot())\n",
        "\n",
        "def named_entity_recognition(text):\n",
        "    entities = []\n",
        "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
        "    for subtree in chunked:\n",
        "        if hasattr(subtree, 'label') and subtree.label() in ['PERSON', 'ORGANIZATION', 'GPE', 'PRODUCT', 'DATE']:\n",
        "            entities.append(' '.join([word for word, _ in subtree]))\n",
        "    entity_count = {entity: entities.count(entity) for entity in set(entities)}\n",
        "    return entity_count\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    text = \"This is an example sentence. It demonstrates the syntax and structure analysis using Python NLTK and spaCy.\"\n",
        "\n",
        "    pos_tags, pos_count = pos_tagging(text)\n",
        "    print(pos_tags)\n",
        "    print(pos_count)\n",
        "\n",
        "    constituency_parsing(text, nlp)\n",
        "    dependency_parsing(text, nlp)\n",
        "\n",
        "    entity_count = named_entity_recognition(text)\n",
        "    print(entity_count)\n"
      ],
      "metadata": {
        "id": "f3ZwfIahBE2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The challenging aspects of the code above may involve understanding and implementing various Natural Language Processing (NLP) techniques like Parts of Speech (POS) tagging, constituency parsing, dependency parsing, and named entity recognition (NER).\n",
        "\n",
        "2. Dealing with the complexities of linguistic phenomena, such as ambiguity, irregularities in grammar, and variations in language usage across different contexts, can be difficult.\n",
        "\n",
        "3. It is important to ensure the accuracy and efficiency of the NLP models, especially when dealing with large volumes of text data and capturing subtle linguistic nuances.\n",
        "\n",
        "4. Handling dependencies and compatibility issues with external libraries like NLTK and spaCy, including installation, setup, and version compatibility, can also be challenging."
      ],
      "metadata": {
        "id": "l6mn0z4_Ba3L"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}